{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae2c8fd3734c812",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787ff0241bf48627",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from healnet.models import HealNet\n",
    "from healnet.etl import MMDataset\n",
    "import torch\n",
    "import einops\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "996657e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 440\n",
      "Validation samples: 67\n",
      "Test samples: 48\n",
      "----- Dataset Sizes -----\n",
      "Train samples: 440\n",
      "Validation samples: 67\n",
      "Test samples: 48\n",
      "\n",
      "--- Train Batch ---\n",
      "Tabular data shape: torch.Size([4, 561])\n",
      "Image data shape: torch.Size([4, 100, 1024])\n",
      "Target shape: torch.Size([4])\n",
      "Tabular data mean: 0.345098614692688, min: -3.238293409347534, max: 10.225213050842285\n",
      "Image data mean: -0.006394848693162203, min: -5.476696014404297, max: 5.192056179046631\n",
      "Target mean: 0.5, min: 0.0, max: 1.0\n",
      "Tabular data type: torch.float32\n",
      "Image data type: torch.float32\n",
      "Target data type: torch.float32\n",
      "\n",
      "--- Validation Batch ---\n",
      "Tabular data shape: torch.Size([4, 561])\n",
      "Image data shape: torch.Size([4, 100, 1024])\n",
      "Target shape: torch.Size([4])\n",
      "Tabular data mean: 0.2190527617931366, min: -2.7998809814453125, max: 5.5618462562561035\n",
      "Image data mean: -0.0029486697167158127, min: -5.746277809143066, max: 4.820747375488281\n",
      "Target mean: 0.5, min: 0.0, max: 1.0\n",
      "Tabular data type: torch.float32\n",
      "Image data type: torch.float32\n",
      "Target data type: torch.float32\n",
      "\n",
      "--- Test Batch ---\n",
      "Tabular data shape: torch.Size([4, 561])\n",
      "Image data shape: torch.Size([4, 100, 1024])\n",
      "Target shape: torch.Size([4])\n",
      "Tabular data mean: -0.22511295974254608, min: -4.1081767082214355, max: 1.7416406869888306\n",
      "Image data mean: -0.006815137341618538, min: -5.469854354858398, max: 5.204044818878174\n",
      "Target mean: 0.5, min: 0.0, max: 1.0\n",
      "Tabular data type: torch.float32\n",
      "Image data type: torch.float32\n",
      "Target data type: torch.float32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, img_features_path, scaler=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_features_path = img_features_path\n",
    "        self.tabular_columns = [col for col in df.columns if col.startswith('cnv')]\n",
    "\n",
    "        self.scaler = scaler\n",
    "        \n",
    "        if self.scaler is not None:\n",
    "            # Ensure the scaler has been fitted\n",
    "            if not hasattr(self.scaler, 'mean_') or not hasattr(self.scaler, 'scale_'):\n",
    "                raise ValueError(\"Scaler must be fitted before being passed to the dataset.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Extract row data\n",
    "        row = self.df.iloc[idx]\n",
    "        wsi_file_name = row[\"wsi_file_name\"][:-5]  # Adjust as needed\n",
    "        target_label = row[\"progressor_status\"]\n",
    "        \n",
    "        # Convert target label from 'NP'/'P' to 0/1\n",
    "        if target_label == 'P':\n",
    "            target = 1\n",
    "        else:\n",
    "            target = 0\n",
    "        \n",
    "        # Load tabular features\n",
    "        tabular_features = row[self.tabular_columns].values.astype('float32')\n",
    "        if self.scaler:\n",
    "            # Apply scaling\n",
    "            tabular_features = self.scaler.transform(tabular_features.reshape(1, -1)).flatten()\n",
    "    \n",
    "        tabular_tensor = torch.tensor(tabular_features, dtype=torch.float32)\n",
    "        \n",
    "        # Load image features from .h5 file\n",
    "        img_features_file = os.path.join(self.img_features_path, wsi_file_name + \".h5\")\n",
    "        with h5py.File(img_features_file, 'r') as h5_file:\n",
    "            # Extract 'cluster_features' dataset\n",
    "            if \"cluster_features\" in h5_file:\n",
    "                cluster_features = h5_file[\"cluster_features\"][:]\n",
    "            else:\n",
    "                raise KeyError(f\"'cluster_features' not found in {img_features_file}\")\n",
    "            \n",
    "        # Convert cluster features to a tensor\n",
    "        img_features_tensor = torch.tensor(cluster_features, dtype=torch.float32)\n",
    "        \n",
    "        # Prepare the target tensor\n",
    "        target_tensor = torch.tensor(target, dtype=torch.float32)\n",
    "        \n",
    "        return {\n",
    "            'tabular': tabular_tensor,\n",
    "            'image': img_features_tensor,\n",
    "            'target': target_tensor\n",
    "        }\n",
    "\n",
    "# Paths to your data\n",
    "path_to_img_features = \"/home/zuberi01/Documents/Development/phd/healnet/h5_files\"\n",
    "path_to_csv = \"/home/zuberi01/Documents/Development/phd/healnet/matching_rows_sequoia.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(path_to_csv)\n",
    "\n",
    "# Split the DataFrame into train, val, and test sets\n",
    "train_df = df[df[\"split_0\"] == \"train\"]\n",
    "val_df = df[df[\"split_0\"] == \"val\"]\n",
    "test_df = df[df[\"split_0\"] == \"test\"]\n",
    "\n",
    "# print the length of each dataset\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Identify tabular columns\n",
    "tabular_columns = [col for col in df.columns if col.startswith('cnv')]\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "# Fit the scaler on the training data\n",
    "scaler.fit(train_df[tabular_columns].values)\n",
    "\n",
    "# Create datasets for each split\n",
    "train_dataset = CustomDataset(train_df, path_to_img_features, scaler=scaler)\n",
    "val_dataset = CustomDataset(val_df, path_to_img_features, scaler=scaler)\n",
    "test_dataset = CustomDataset(test_df, path_to_img_features, scaler=scaler)\n",
    "\n",
    "# Define loader arguments\n",
    "loader_args = {\n",
    "    \"batch_size\": 4,  # Adjust batch size as needed\n",
    "    \"shuffle\": True, \n",
    "    \"num_workers\": 8, \n",
    "    \"pin_memory\": True, \n",
    "    \"multiprocessing_context\": \"fork\", \n",
    "    \"persistent_workers\": True, \n",
    "}\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, **loader_args)\n",
    "val_loader = DataLoader(val_dataset, **loader_args)\n",
    "test_loader = DataLoader(test_dataset, **loader_args)\n",
    "\n",
    "# ------------------ Data Validation Tests ------------------\n",
    "\n",
    "def validate_datasets(train_loader, val_loader, test_loader):\n",
    "    print(\"----- Dataset Sizes -----\")\n",
    "    print(f\"Train samples: {len(train_loader.dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "    print(f\"Test samples: {len(test_loader.dataset)}\\n\")\n",
    "    \n",
    "    # Function to validate a single batch\n",
    "    def validate_batch(batch, split_name):\n",
    "        tabular = batch['tabular']\n",
    "        image = batch['image']\n",
    "        target = batch['target']\n",
    "        \n",
    "        print(f\"--- {split_name} Batch ---\")\n",
    "        print(f\"Tabular data shape: {tabular.shape}\")  # Expected: (batch_size, tabular_features)\n",
    "        print(f\"Image data shape: {image.shape}\")      # Depends on your image feature dimensions\n",
    "        print(f\"Target shape: {target.shape}\")        # Expected: (batch_size,)\n",
    "\n",
    "        # get the mean, min, and max values of the tabular data\n",
    "        print(f\"Tabular data mean: {tabular.mean()}, min: {tabular.min()}, max: {tabular.max()}\")\n",
    "        # get the mean, min, and max values of the image data\n",
    "        print(f\"Image data mean: {image.mean()}, min: {image.min()}, max: {image.max()}\")\n",
    "        # get the mean, min, and max values of the target data\n",
    "        print(f\"Target mean: {target.mean()}, min: {target.min()}, max: {target.max()}\")\n",
    "        \n",
    "        print(f\"Tabular data type: {tabular.dtype}\")\n",
    "        print(f\"Image data type: {image.dtype}\")\n",
    "        print(f\"Target data type: {target.dtype}\\n\")\n",
    "        \n",
    "        # Optionally, visualize the first image in the batch\n",
    "        # Uncomment the following lines if image data is suitable for visualization\n",
    "        \"\"\"\n",
    "        if image.ndim == 3:  # Assuming image shape is (batch_size, channels, features)\n",
    "            img = image[0].numpy()\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            plt.title(f\"{split_name} Sample Image\")\n",
    "            plt.show()\n",
    "        \"\"\"\n",
    "\n",
    "    # Get one batch from each DataLoader\n",
    "    train_batch = next(iter(train_loader))\n",
    "    val_batch = next(iter(val_loader))\n",
    "    test_batch = next(iter(test_loader))\n",
    "    \n",
    "    validate_batch(train_batch, \"Train\")\n",
    "    validate_batch(val_batch, \"Validation\")\n",
    "    validate_batch(test_batch, \"Test\")\n",
    "\n",
    "# Run the validation\n",
    "validate_datasets(train_loader, val_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ee2478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(x='progressor_status', data=df)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[tabular_columns] = scaler.fit_transform(df[tabular_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27057d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution for Training Set:\n",
      "    Count  Percentage\n",
      "NP    291   66.136364\n",
      "P     149   33.863636\n",
      "\n",
      "\n",
      "Class Distribution for Validation Set:\n",
      "    Count  Percentage\n",
      "NP     41    61.19403\n",
      "P      26    38.80597\n",
      "\n",
      "\n",
      "Class Distribution for Test Set:\n",
      "    Count  Percentage\n",
      "NP     25   52.083333\n",
      "P      23   47.916667\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_class_distribution(df, split_name, target_column):\n",
    "    \"\"\"\n",
    "    Prints the count and percentage of each class in the given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to analyze.\n",
    "        split_name (str): Name of the split (e.g., 'Training', 'Validation', 'Test').\n",
    "        target_column (str): The name of the target column.\n",
    "    \"\"\"\n",
    "    counts = df[target_column].value_counts()\n",
    "    percentages = df[target_column].value_counts(normalize=True) * 100\n",
    "    distribution = pd.concat([counts, percentages], axis=1)\n",
    "    distribution.columns = ['Count', 'Percentage']\n",
    "    print(f\"Class Distribution for {split_name} Set:\")\n",
    "    print(distribution)\n",
    "    print(\"\\n\")\n",
    "\n",
    "target_column = 'progressor_status'\n",
    "print_class_distribution(train_df, 'Training', target_column)\n",
    "print_class_distribution(val_df, 'Validation', target_column)\n",
    "print_class_distribution(test_df, 'Test', target_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbee4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuberi01/mambaforge/envs/cognition/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Sample image shape: torch.Size([100, 1024])\n",
      "tabular_features 561\n",
      "image_features 100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1196 | Train Acc: 51.14%\n",
      "Val Loss: 0.7152 | Val Acc: 61.19%\n",
      "No improvement. Counter: 1/10\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9195 | Train Acc: 49.77%\n",
      "Val Loss: 1.3022 | Val Acc: 61.19%\n",
      "No improvement. Counter: 2/10\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3962 | Train Acc: 54.32%\n",
      "Val Loss: 2.8749 | Val Acc: 61.19%\n",
      "No improvement. Counter: 3/10\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3049 | Train Acc: 50.23%\n",
      "Val Loss: 0.7034 | Val Acc: 61.19%\n",
      "No improvement. Counter: 4/10\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5248 | Train Acc: 53.41%\n",
      "Val Loss: 4.2587 | Val Acc: 61.19%\n",
      "No improvement. Counter: 5/10\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0127 | Train Acc: 52.05%\n",
      "Val Loss: 0.7103 | Val Acc: 61.19%\n",
      "No improvement. Counter: 6/10\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8110 | Train Acc: 48.41%\n",
      "Val Loss: 0.7743 | Val Acc: 38.81%\n",
      "No improvement. Counter: 7/10\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8665 | Train Acc: 55.68%\n",
      "Val Loss: 0.7001 | Val Acc: 61.19%\n",
      "No improvement. Counter: 8/10\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7590 | Train Acc: 52.50%\n",
      "Val Loss: 0.7229 | Val Acc: 38.81%\n",
      "No improvement. Counter: 9/10\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7752 | Train Acc: 52.05%\n",
      "Val Loss: 0.7118 | Val Acc: 38.81%\n",
      "No improvement. Counter: 10/10\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6669 | Test Acc: 47.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# ------------------ Model Training ------------------\n",
    "\n",
    "from healnet.models.healnet import HealNet  # Adjust import path as necessary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm  # For progress bars\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 2  # Adjust based on your dataset\n",
    "\n",
    "# Inspect the shape of image features to set input dimensions\n",
    "# Assuming all image features have the same shape\n",
    "sample_image = train_dataset[0]['image']\n",
    "print(f\"Sample image shape: {sample_image.shape}\")  # Example: (channels, features)\n",
    "\n",
    "# Example: If cluster_features have shape (channels, features), you might need to flatten or process them\n",
    "# Adjust HealNet parameters accordingly\n",
    "\n",
    "# Determine input dimensions\n",
    "# For tabular data\n",
    "tabular_features = train_dataset[0]['tabular'].shape[0]  # 5000 as per your data\n",
    "\n",
    "# For image data\n",
    "# Assuming image features are 1D vectors per sample; adjust if different\n",
    "image_features = train_dataset[0]['image'].shape[0]  # Number of image feature channels\n",
    "# If image features are multi-dimensional, adjust input_axes and potentially the model architecture\n",
    "\n",
    "# Instantiate the HealNet model with corrected input_axes and input_channels\n",
    "model = HealNet(\n",
    "    modalities=2, \n",
    "    input_channels=[1, 100],  # 1 for tabular, 100 for image features\n",
    "    input_axes=[1, 2],        # 1 axis for tabular, 2 axes for image features\n",
    "    num_classes=num_classes,\n",
    "    # Add other parameters as required by your model configuration\n",
    ").to(device)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_df['progressor_status']),\n",
    "    y=train_df['progressor_status']\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "print('tabular_features', tabular_features)\n",
    "print('image_features', image_features)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "# Define the scheduler\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#    optimizer,\n",
    "#    mode='min',       # Since we want to minimize validation loss\n",
    "#    factor=0.1,       # Reduce LR by a factor of 0.1\n",
    "#    patience=3,       # Number of epochs with no improvement after which to reduce LR\n",
    "#    verbose=True      # Print messages when LR is updated\n",
    "#)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 20  # Adjust based on your needs\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(train_loader)\n",
    "\n",
    "# Define the OneCycleLR scheduler\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer=optimizer,\n",
    "    max_lr=0.05,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=num_epochs,\n",
    "    anneal_strategy='linear',  # or 'cos', depending on preference\n",
    "    pct_start=0.3,             # percentage of cycle spent increasing LR\n",
    "    div_factor=10.0,           # initial lr = max_lr/div_factor\n",
    "    final_div_factor=1e4,      # minimum lr = max_lr/(div_factor*final_div_factor)\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "# Lists to store metrics for visualization\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "\n",
    "# Training and Validation Loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "    \n",
    "    # ------------------ Training Phase ------------------\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Example usage: iterate over the training data\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):\n",
    "\n",
    "        tabular_data = batch['tabular'].to(device)  # Shape: (batch_size, tabular_features)\n",
    "        image_data = batch['image'].to(device)      # Shape: (batch_size, channels, features)\n",
    "        targets = batch['target'].long().to(device) # Shape: (batch_size,)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model([tabular_data, image_data])  # Shape: (batch_size, num_classes)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        train_loss += loss.item() * tabular_data.size(0)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        # ------------------ Scheduler Step ------------------\n",
    "        scheduler.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # ------------------ Validation Phase ------------------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            tabular_data = batch['tabular'].to(device)\n",
    "            image_data = batch['image'].to(device)\n",
    "            targets = batch['target'].long().to(device)\n",
    "\n",
    "            outputs = model([tabular_data, image_data])\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * tabular_data.size(0)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    # ------------------ Checkpointing ------------------\n",
    "    #if avg_val_loss < best_val_loss:\n",
    "    #    best_val_loss = avg_val_loss\n",
    "    #    torch.save(model.state_dict(), '/scratchc/fmlab/zuberi01/phd/healnet/notebooks/best_healnet_model.pth')\n",
    "    #    print(\"âœ… Model saved!\")\n",
    "    #    counter = 0\n",
    "    #else:\n",
    "    counter += 1\n",
    "    print(f\"No improvement. Counter: {counter}/{patience}\")\n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# ------------------ Testing Phase ------------------\n",
    "\n",
    "# Load the best model\n",
    "# model.load_state_dict(torch.load('best_healnet_model.pth'), weights_only=False)\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
    "        tabular_data = batch['tabular'].to(device)\n",
    "        image_data = batch['image'].to(device)\n",
    "        targets = batch['target'].long().to(device)\n",
    "        \n",
    "        outputs = model([tabular_data, image_data])\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * tabular_data.size(0)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "test_accuracy = 100 * correct / total\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss:.4f} | Test Acc: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de637307d506881",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Synthetic modalities\n",
    "\n",
    "We instantiate a synthetic multimodal dataset for demo purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a24d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "769f89f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratchc/fmlab/zuberi01/phd/sequoia-pub/examples/matching_rows_sequoia.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m path_to_csv \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/scratchc/fmlab/zuberi01/phd/sequoia-pub/examples/matching_rows_sequoia.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Load the CSV file\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_csv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Split the DataFrame into train, val, and test sets\u001b[39;00m\n\u001b[1;32m     62\u001b[0m train_df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_0\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/cognition/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/cognition/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/cognition/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/cognition/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/mambaforge/envs/cognition/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/cognition/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/mambaforge/envs/cognition/lib/python3.9/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/scratchc/fmlab/zuberi01/phd/sequoia-pub/examples/matching_rows_sequoia.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define the custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, img_features_path):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_features_path = img_features_path\n",
    "        self.tabular_columns = [col for col in df.columns if col.startswith('cnv')]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Extract row data\n",
    "        row = self.df.iloc[idx]\n",
    "        wsi_file_name = row[\"wsi_file_name\"][:-5]  # Adjust as needed\n",
    "        target_label = row[\"progressor_status\"]\n",
    "        \n",
    "        # Convert target label from 'NP'/'P' to 0/1\n",
    "        if target_label == 'P':\n",
    "            target = 1\n",
    "        else:\n",
    "            target = 0\n",
    "        \n",
    "        # Load tabular features\n",
    "        tabular_features = row[self.tabular_columns].values.astype('float32')\n",
    "        tabular_tensor = torch.tensor(tabular_features)\n",
    "        \n",
    "        # Load image features from .h5 file\n",
    "        img_features_file = os.path.join(self.img_features_path, wsi_file_name + \".h5\")\n",
    "        with h5py.File(img_features_file, 'r') as h5_file:\n",
    "            # Extract 'cluster_features' dataset\n",
    "            if \"cluster_features\" in h5_file:\n",
    "                cluster_features = h5_file[\"cluster_features\"][:]\n",
    "            else:\n",
    "                raise KeyError(f\"'cluster_features' not found in {img_features_file}\")\n",
    "            \n",
    "        # Convert cluster features to a tensor\n",
    "        img_features_tensor = torch.tensor(cluster_features, dtype=torch.float32)\n",
    "        \n",
    "        # Prepare the target tensor\n",
    "        target_tensor = torch.tensor(target, dtype=torch.float32)\n",
    "        \n",
    "        return {\n",
    "            'tabular': tabular_tensor,\n",
    "            'image': img_features_tensor,\n",
    "            'target': target_tensor\n",
    "        }\n",
    "\n",
    "# Paths to your data\n",
    "path_to_img_features = \"/scratchc/fmlab/zuberi01/masters/saved_patches/40x_400/features2/h5_files/\"\n",
    "path_to_csv = \"/scratchc/fmlab/zuberi01/phd/sequoia-pub/examples/matching_rows_sequoia.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(path_to_csv)\n",
    "\n",
    "# Split the DataFrame into train, val, and test sets\n",
    "train_df = df[df[\"split_0\"] == \"train\"]\n",
    "val_df = df[df[\"split_0\"] == \"val\"]\n",
    "test_df = df[df[\"split_0\"] == \"test\"]\n",
    "\n",
    "# Create datasets for each split\n",
    "train_dataset = CustomDataset(train_df, path_to_img_features)\n",
    "val_dataset = CustomDataset(val_df, path_to_img_features)\n",
    "test_dataset = CustomDataset(test_df, path_to_img_features)\n",
    "\n",
    "# Define loader arguments\n",
    "loader_args = {\n",
    "    \"batch_size\": 4,  # Adjust batch size as needed\n",
    "    \"shuffle\": True, \n",
    "    \"num_workers\": 8, \n",
    "    \"pin_memory\": True, \n",
    "    \"multiprocessing_context\": \"fork\", \n",
    "    \"persistent_workers\": True, \n",
    "}\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, **loader_args)\n",
    "val_loader = DataLoader(val_dataset, **loader_args)\n",
    "test_loader = DataLoader(test_dataset, **loader_args)\n",
    "\n",
    "# Example usage: iterate over the training data\n",
    "for batch in train_loader:\n",
    "    tabular_data = batch['tabular']\n",
    "    image_data = batch['image']\n",
    "    targets = batch['target']\n",
    "    \n",
    "    # Now you can feed tabular_data and image_data into your model\n",
    "    # For example:\n",
    "    # outputs = model(tabular_data, image_data)\n",
    "    # loss = loss_fn(outputs, targets)\n",
    "    # ... (rest of your training loop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "023d5b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the HealNet class\n",
    "from healnet.models.healnet import HealNet\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 2  # Adjust based on your dataset\n",
    "\n",
    "# Instantiate the HealNet model\n",
    "model = HealNet(\n",
    "    modalities=2, \n",
    "    input_channels=[1, 1],  # Number of channels per modality\n",
    "    input_axes=[1, 2],       # Updated to match the number of axes per modality\n",
    "    num_classes=num_classes,\n",
    "    # Add other parameters as required by your model configuration\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c17c5f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All input modalities have the correct number of axes.\n"
     ]
    }
   ],
   "source": [
    "def check_input_axes(inputs, expected_axes):\n",
    "    \"\"\"\n",
    "    Checks if each input modality has the expected number of axes.\n",
    "\n",
    "    Parameters:\n",
    "    - inputs (list of torch.Tensor): The input tensors for each modality.\n",
    "    - expected_axes (list of int): The expected number of axes for each modality.\n",
    "\n",
    "    Raises:\n",
    "    - AssertionError: If any modality does not have the expected number of axes.\n",
    "    \"\"\"\n",
    "    for i, (tensor, expected) in enumerate(zip(inputs, expected_axes), 1):\n",
    "        # Exclude the batch dimension\n",
    "        actual_axes = tensor.dim() - 2  # Assuming [batch, channels, ...]\n",
    "        assert actual_axes == expected, (\n",
    "            f\"Input data for modality {i} has {actual_axes} axes, \"\n",
    "            f\"but expected {expected} axes.\"\n",
    "        )\n",
    "    print(\"All input modalities have the correct number of axes.\")\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# Sample shapes based on your error\n",
    "# Tabular Data: [batch_size, channels, num_tabular_features] => [4, 1, 561]\n",
    "tabular_data = torch.rand(4, 1, 561)\n",
    "\n",
    "# Image Data: [batch_size, channels, height, width] => [4, 1, 100, 1024]\n",
    "image_data = torch.rand(4, 1, 100, 1024)\n",
    "\n",
    "# Combine into a list\n",
    "inputs = [tabular_data, image_data]\n",
    "\n",
    "# Define expected axes\n",
    "expected_axes = [1, 2]\n",
    "\n",
    "\n",
    "try:\n",
    "    check_input_axes(inputs, expected_axes)\n",
    "except AssertionError as e:\n",
    "    print(f\"AssertionError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee41ba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 3/110 [00:00<00:01, 83.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 561])\n",
      "torch.Size([4, 1, 100, 1024])\n",
      "torch.Size([4])\n",
      "torch.Size([4, 1, 561])\n",
      "torch.Size([4, 1, 100, 1024])\n",
      "torch.Size([4])\n",
      "torch.Size([4, 1, 561])\n",
      "torch.Size([4, 1, 100, 1024])\n",
      "torch.Size([4])\n",
      "torch.Size([4, 1, 561])\n",
      "torch.Size([4, 1, 100, 1024])\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(tqdm(train_loader)):\n",
    "    # Retrieve data and move to device\n",
    "    tabular_data = batch['tabular'].to(device).unsqueeze(1)  # Shape: [batch_size, 1, num_tabular_features]\n",
    "    image_data = batch['image'].to(device).unsqueeze(1)      # Shape: [batch_size, 1, num_image_features]\n",
    "    targets = batch['target'].long().to(device) \n",
    "\n",
    "    # print their shapes\n",
    "    print(tabular_data.shape)\n",
    "    print(image_data.shape)\n",
    "    print(targets.shape)\n",
    "\n",
    "    # break after printing the first three\n",
    "    if batch_idx == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ccc5c171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "HealNet model instantiated successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Batch 1\n",
      "Adjusted Tabular Data Shape: torch.Size([4, 1, 561])\n",
      "Adjusted Image Data Shape: torch.Size([4, 1, 100, 1024])\n",
      "Targets Shape: torch.Size([4])\n",
      "inputs size\n",
      "2\n",
      "torch.Size([4, 1, 561]) torch.Size([4, 1, 100, 1024])\n",
      "2\n",
      "len of tensors: 2\n",
      "shape of tensor[0]: torch.Size([4, 1, 561])\n",
      "shape of tensor[1]: torch.Size([4, 1, 100, 1024])\n",
      "shape of data: torch.Size([4, 1, 561])\n",
      "assert passed\n",
      "len of tensors: 2\n",
      "shape of tensor[0]: torch.Size([4, 1, 566])\n",
      "shape of tensor[1]: torch.Size([4, 1, 100, 1024])\n",
      "shape of data: torch.Size([4, 1, 100, 1024])\n",
      "assert passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 4 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, inputs[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 60\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m                       \u001b[38;5;66;03m# Expected Shape: [batch_size, num_classes]\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone outputs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Debugging: Print outputs shape and sample outputs for the first batch\u001b[39;00m\n",
      "File \u001b[0;32m/home/zuberi01/miniforge3/envs/healnetenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zuberi01/miniforge3/envs/healnetenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/scratchc/fmlab/zuberi01/phd/healnet/healnet/models/healnet.py:301\u001b[0m, in \u001b[0;36mHealNet.forward\u001b[0;34m(self, tensors, mask, missing, return_embeddings)\u001b[0m\n\u001b[1;32m    299\u001b[0m     enc_pos \u001b[38;5;241m=\u001b[39m rearrange(enc_pos, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn d -> () n d\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    300\u001b[0m     enc_pos \u001b[38;5;241m=\u001b[39m repeat(enc_pos, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m() n d -> b n d\u001b[39m\u001b[38;5;124m'\u001b[39m, b \u001b[38;5;241m=\u001b[39m b)\n\u001b[0;32m--> 301\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# concat and flatten axis for each modality\u001b[39;00m\n\u001b[1;32m    304\u001b[0m data \u001b[38;5;241m=\u001b[39m rearrange(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb ... d -> b (...) d\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 4 and 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from healnet.models.healnet import HealNet\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 2  # Adjust based on your dataset\n",
    "\n",
    "# Instantiate the HealNet model with updated input_axes\n",
    "model = HealNet(\n",
    "    modalities=2, \n",
    "    input_channels=[1, 1],  # Number of channels per modality\n",
    "    input_axes=[1, 2],       # Updated to match the number of axes per modality\n",
    "    num_classes=num_classes,\n",
    "    # Add other parameters as required by your model configuration\n",
    ").to(device)\n",
    "\n",
    "print(\"HealNet model instantiated successfully.\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10  # Adjust as needed\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader)):\n",
    "        # Retrieve data and move to device\n",
    "        tabular_data = batch['tabular'].to(device).unsqueeze(1)  # Shape: [batch_size, 1, num_tabular_features]\n",
    "        image_data = batch['image'].to(device).unsqueeze(1)      # Shape: [batch_size, 1, num_image_features]\n",
    "        targets = batch['target'].long().to(device)             # Shape: [batch_size]\n",
    "\n",
    "        # Debugging: Print shapes for the first batch of each epoch\n",
    "        if batch_idx == 0 or batch_idx == 1 or batch_idx == 2:\n",
    "            print(f\"\\nEpoch {epoch+1}, Batch {batch_idx+1}\")\n",
    "            print(f\"Adjusted Tabular Data Shape: {tabular_data.shape}\")\n",
    "            print(f\"Adjusted Image Data Shape: {image_data.shape}\")\n",
    "            print(f\"Targets Shape: {targets.shape}\")\n",
    "\n",
    "        # Prepare inputs\n",
    "        inputs = [tabular_data, image_data]\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        print('inputs size')\n",
    "        print(len(inputs))\n",
    "        print(inputs[0].shape, inputs[1].shape)\n",
    "\n",
    "        outputs = model(inputs)                       # Expected Shape: [batch_size, num_classes]\n",
    "        print('done outputs')\n",
    "\n",
    "        # Debugging: Print outputs shape and sample outputs for the first batch\n",
    "        if batch_idx == 0:\n",
    "            print(f\"Outputs Shape: {outputs.shape}\")\n",
    "            print(f\"Outputs Sample: {outputs[:2]}\")\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_batch_idx, batch in enumerate(val_loader):\n",
    "            tabular_data = batch['tabular'].to(device).unsqueeze(1)  # Shape: [batch_size, 1, num_tabular_features]\n",
    "            image_data = batch['image'].to(device).unsqueeze(1)      # Shape: [batch_size, 1, num_image_features]\n",
    "            targets = batch['target'].long().to(device)             # Shape: [batch_size]\n",
    "\n",
    "            # Prepare inputs\n",
    "            inputs = [tabular_data, image_data]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            val_loss = loss_fn(outputs, targets)\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "            # Debugging: Print validation outputs shape and sample outputs for the first batch\n",
    "            if val_batch_idx == 0:\n",
    "                print(f\"Validation Outputs Shape: {outputs.shape}\")\n",
    "                print(f\"Validation Outputs Sample: {outputs[:2]}\")\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {val_epoch_loss:.4f}\\n\")\n",
    "\n",
    "# After training, evaluate on the test set\n",
    "model.eval()\n",
    "test_running_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for test_batch_idx, batch in enumerate(tqdm(test_loader)):\n",
    "        tabular_data = batch['tabular'].to(device)  # Shape: [batch_size, 1, num_tabular_features]\n",
    "        image_data = batch['image'].to(device)     # Shape: [batch_size, 1, num_image_features]\n",
    "        targets = batch['target'].long().to(device)             # Shape: [batch_size]\n",
    "\n",
    "        # Prepare inputs\n",
    "        inputs = [tabular_data, image_data]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        test_loss = loss_fn(outputs, targets)\n",
    "        test_running_loss += test_loss.item()\n",
    "\n",
    "        # Compute predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "        # Debugging: Print test outputs shape and sample outputs for the first batch\n",
    "        if test_batch_idx == 0:\n",
    "            print(f\"Test Outputs Shape: {outputs.shape}\")\n",
    "            print(f\"Test Outputs Sample: {outputs[:2]}\")\n",
    "\n",
    "test_epoch_loss = test_running_loss / len(test_loader)\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Loss: {test_epoch_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1350459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1df31a7b4a69e7aa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = MMDataset([tab_tensor, img_tensor], target)\n",
    "train, test, val = torch.utils.data.random_split(data, [0.7, 0.15, 0.15]) # create 70-15-15 train-val-test split\n",
    "\n",
    "loader_args = {\n",
    "    \"shuffle\": True, \n",
    "    \"num_workers\": 8, \n",
    "    \"pin_memory\": True, \n",
    "    \"multiprocessing_context\": \"fork\", \n",
    "    \"persistent_workers\": True, \n",
    "}\n",
    "\n",
    "train_loader = DataLoader(train, **loader_args)\n",
    "val_loader = DataLoader(val, **loader_args)\n",
    "test_loader = DataLoader(test, **loader_args)\n",
    "# example use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45ab1e79c8bc3a21",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example use\n",
    "[tab_sample, img_sample], target = data[0]\n",
    "\n",
    "# emulate batch dimension\n",
    "tab_sample = einops.repeat(tab_sample, 'c d -> b c d', b=1)\n",
    "img_sample = einops.repeat(img_sample, 'c h w -> b c (h w)', b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38724a29f4142f8e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 262144])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "504d15c029cf68e0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = HealNet(\n",
    "            modalities=2, \n",
    "            input_channels=[tab_c, img_c], \n",
    "            input_axes=[1, 1], # channel axes (0-indexed)\n",
    "            num_classes = n_classes,  \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9e77fe00f07c904",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9904, 0.2519]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "model([tab_sample, img_sample])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
